{"componentChunkName":"component---src-templates-blog-post-js","path":"/analysis-updates/perils-of-automatically-collecting-covid-data","result":{"data":{"contentfulBlogPost":{"contentful_id":"6klBN04FVKSXfzNT93rm3J","title":"(The Perils of) Automatically Collecting COVID Data ","updateDateTime":null,"twitterText":null,"authors":[{"name":"Theo Michel","twitterLink":"https://twitter.com/theotayo","twitterHandle":"theotayo","link":null,"childContentfulAuthorBiographyTextNode":{"childMarkdownRemark":{"html":"<p>Theo Michel is a Project Manager and contributor to Data Quality efforts at the Covid Tracking Project.</p>"}},"headshot":{"file":{"fileName":"00100sPORTRAIT_00100_BURST20191228212428376_COVER4.jpeg"},"resize":{"width":200,"height":200,"src":"//images.ctfassets.net/o2ll9t4ee8tq/7MDmEiBhy8tbupgfoGHUxX/fc97cac5c6b3a195e52ba5f045576128/00100sPORTRAIT_00100_BURST20191228212428376_COVER4.jpeg?w=200&fl=progressive&q=50"}}},{"name":"Rebma","twitterLink":null,"twitterHandle":null,"link":null,"childContentfulAuthorBiographyTextNode":{"childMarkdownRemark":{"html":"<p>Rebma is an infrastructure engineer who likes puzzles, solving problems, and who did the data scripting and automation for The Covid Tracking Project.</p>"}},"headshot":{"file":{"fileName":"mob_tome_square.png"},"resize":{"width":200,"height":200,"src":"//images.ctfassets.net/o2ll9t4ee8tq/224EhYZvbAAv2Ld2IqfuTn/e223961d2702b39ba82dc52bf6ed67a6/mob_tome_square.png?w=200&fl=progressive&q=50"}}}],"socialCard":null,"relatedBlogPosts":null,"childContentfulBlogPostFootnotesTextNode":null,"categories":[{"name":"How We Made The COVID Tracking Project","slug":"how-we-made-the-project","blog_post":[{"slug":"why-we-didnt-automate-our-data-collection","publishDate":"May 28, 2021","authors":[{"name":"Jonathan Gilmour","twitterLink":null,"twitterHandle":null,"link":null,"childContentfulAuthorBiographyTextNode":{"childMarkdownRemark":{"html":"<p>Jonathan Gilmour is a Data Infrastructure engineer and Data Quality contributor at The COVID Tracking Project.</p>"}},"headshot":{"file":{"fileName":"IMG_2198.jpg"},"resize":{"width":200,"height":199,"src":"//images.ctfassets.net/o2ll9t4ee8tq/3czVgHQ6fsml4UEePyyOcA/5963a9b5d839552cde3e382c12de2334/IMG_2198.jpg?w=200&fl=progressive&q=50"}}}],"title":"20,000 Hours of Data Entry: Why We Didn’t Automate Our Data Collection","lede":{"lede":"Looking back on a year of collecting COVID-19 data, here’s a summary of the tools we automated to make our data entry smoother and why we ultimately relied on manual data collection.\n"}},{"slug":"how-we-used-multiple-dating-schemes-to-provide-the-most-complete-picture-of-the-pandemic","publishDate":"May 13, 2021","authors":[{"name":"Theo Michel","twitterLink":"https://twitter.com/theotayo","twitterHandle":"theotayo","link":null,"childContentfulAuthorBiographyTextNode":{"childMarkdownRemark":{"html":"<p>Theo Michel is a Project Manager and contributor to Data Quality efforts at the Covid Tracking Project.</p>"}},"headshot":{"file":{"fileName":"00100sPORTRAIT_00100_BURST20191228212428376_COVER4.jpeg"},"resize":{"width":200,"height":200,"src":"//images.ctfassets.net/o2ll9t4ee8tq/7MDmEiBhy8tbupgfoGHUxX/fc97cac5c6b3a195e52ba5f045576128/00100sPORTRAIT_00100_BURST20191228212428376_COVER4.jpeg?w=200&fl=progressive&q=50"}}},{"name":"Rebma","twitterLink":null,"twitterHandle":null,"link":null,"childContentfulAuthorBiographyTextNode":{"childMarkdownRemark":{"html":"<p>Rebma is an infrastructure engineer who likes puzzles, solving problems, and who did the data scripting and automation for The Covid Tracking Project.</p>"}},"headshot":{"file":{"fileName":"mob_tome_square.png"},"resize":{"width":200,"height":200,"src":"//images.ctfassets.net/o2ll9t4ee8tq/224EhYZvbAAv2Ld2IqfuTn/e223961d2702b39ba82dc52bf6ed67a6/mob_tome_square.png?w=200&fl=progressive&q=50"}}}],"title":"Dating Data: How We Used Multiple Dating Schemes to Provide the Most Complete Picture of the Pandemic","lede":{"lede":"Throughout our year of tracking COVID-19 tests, cases, and outcomes, we were confronted with data organized by numerous dating schemes. Here’s how we came to understand those dating schemes, and the solution we developed for making the best of them.\n"}},{"slug":"how-why-covid-tracking-project-built-screenshot-system","publishDate":"May 4, 2021","authors":[{"name":"Julia Kodysh","twitterLink":"https://twitter.com/JuliaKodysh","twitterHandle":"JuliaKodysh","link":null,"childContentfulAuthorBiographyTextNode":{"childMarkdownRemark":{"html":"<p>Julia Kodysh is Data Infrastructure co-lead at The COVID Tracking Project.</p>"}},"headshot":{"file":{"fileName":"Julia Kodysh.jpg"},"resize":{"width":200,"height":200,"src":"//images.ctfassets.net/o2ll9t4ee8tq/i8OkSEqUPR54FtnXdXTUa/669cc104b9ad835791b48e7889bac084/Julia_Kodysh.jpg?w=200&fl=progressive&q=50"}}},{"name":"Jonathan Gilmour","twitterLink":null,"twitterHandle":null,"link":null,"childContentfulAuthorBiographyTextNode":{"childMarkdownRemark":{"html":"<p>Jonathan Gilmour is a Data Infrastructure engineer and Data Quality contributor at The COVID Tracking Project.</p>"}},"headshot":{"file":{"fileName":"IMG_2198.jpg"},"resize":{"width":200,"height":199,"src":"//images.ctfassets.net/o2ll9t4ee8tq/3czVgHQ6fsml4UEePyyOcA/5963a9b5d839552cde3e382c12de2334/IMG_2198.jpg?w=200&fl=progressive&q=50"}}}],"title":"How and Why The COVID Tracking Project Built a Screenshot System","lede":{"lede":"A system for regularly capturing static images of state COVID-19 websites helped us produce an archive and verify our published data.\n"}},{"slug":"behind-the-help-desk","publishDate":"May 3, 2021","authors":[{"name":"Amanda French","twitterLink":"https://twitter.com/amandafrench","twitterHandle":"amandafrench","link":"https://amandafrench.net","childContentfulAuthorBiographyTextNode":{"childMarkdownRemark":{"html":"<p>Amanda French, Community Lead and Data Entry Shift Lead at The COVID Tracking Project, has a doctorate in English and is an expert in digital humanities.</p>"}},"headshot":{"file":{"fileName":"amanda_vt_square.jpg"},"resize":{"width":200,"height":200,"src":"//images.ctfassets.net/o2ll9t4ee8tq/59iE658OcItKmQZ1z5R6sy/37cf4300944f3d888fa1e461982b4b0b/amanda_vt_square.jpg?w=200&fl=progressive&q=50"}}},{"name":"Brian S.-K. Li","twitterLink":null,"twitterHandle":null,"link":"https://www.linkedin.com/in/brian-s-k-li-b38b6217a/","childContentfulAuthorBiographyTextNode":{"childMarkdownRemark":{"html":"<p>Brian S.-K. Li has been a Data Entry and Data Quality Shift Lead at The COVID Tracking Project since May 2020 and is a student at Princeton University.</p>"}},"headshot":{"file":{"fileName":"1602448073512-1.jpg"},"resize":{"width":200,"height":200,"src":"//images.ctfassets.net/o2ll9t4ee8tq/48ijMZs2LalletcIJgPOLz/cdfc8b27a288f0a74b933f72bcbeb9f9/1602448073512-1.jpg?w=200&fl=progressive&q=50"}}}],"title":"Behind The COVID Tracking Project’s Public Help Desk","lede":{"lede":"Volunteers and staffers at The COVID Tracking Project replied to thousands of messages from the public last year. Here’s why we took the trouble, and here’s what people wanted to know. "}},{"slug":"inside-the-covid-tracking-projects-volunteer-organization","publishDate":"April 22, 2021","authors":[{"name":"Amanda French","twitterLink":"https://twitter.com/amandafrench","twitterHandle":"amandafrench","link":"https://amandafrench.net","childContentfulAuthorBiographyTextNode":{"childMarkdownRemark":{"html":"<p>Amanda French, Community Lead and Data Entry Shift Lead at The COVID Tracking Project, has a doctorate in English and is an expert in digital humanities.</p>"}},"headshot":{"file":{"fileName":"amanda_vt_square.jpg"},"resize":{"width":200,"height":200,"src":"//images.ctfassets.net/o2ll9t4ee8tq/59iE658OcItKmQZ1z5R6sy/37cf4300944f3d888fa1e461982b4b0b/amanda_vt_square.jpg?w=200&fl=progressive&q=50"}}},{"name":"Nicki Camberg","twitterLink":"https://twitter.com/nickicamberg","twitterHandle":"nickicamberg","link":null,"childContentfulAuthorBiographyTextNode":{"childMarkdownRemark":{"html":"<p>Nicki Camberg is a student journalist studying Political Science and Statistics at Barnard College, and the City Data Manager at CTP.</p>"}},"headshot":{"file":{"fileName":"FullSizeRender.jpeg"},"resize":{"width":200,"height":200,"src":"//images.ctfassets.net/o2ll9t4ee8tq/1fDS53I3BgVtZXFohdrAyJ/dddc911a7ce11730f1a33bb2c936d182/FullSizeRender.jpeg?w=200&fl=progressive&q=50"}}}],"title":"Inside The COVID Tracking Project's Volunteer Organization","lede":{"lede":"More than 800 volunteers performed thousands of hours of work to make our regular data operation possible during the COVID-19 pandemic."}},{"slug":"measuring-our-impact","publishDate":"April 14, 2021","authors":[{"name":"Rachel Glickhouse","twitterLink":"https://twitter.com/Riogringa","twitterHandle":"Riogringa","link":"https://www.rachelglickhouse.com/","childContentfulAuthorBiographyTextNode":{"childMarkdownRemark":{"html":"<p>Rachel Glickhouse is a New York-based journalist who works on project management and impact tracking at The COVID Tracking Project.</p>"}},"headshot":{"file":{"fileName":"HeadshotNew18 (2).jpg"},"resize":{"width":200,"height":200,"src":"//images.ctfassets.net/o2ll9t4ee8tq/1ayF6qqGyR64oDgVrwWBhO/5b41360fe7c3984506b78c3d8e425d51/HeadshotNew18__2_.jpg?w=200&fl=progressive&q=50"}}}],"title":"Measuring Our Impact at The COVID Tracking Project","lede":{"lede":"Our largely volunteer-operated effort became a critical data source for journalists, scientists, academics and government officials."}},{"slug":"the-decisions-we-made","publishDate":"March 31, 2021","authors":[{"name":"Erin Kissane","twitterLink":"https://twitter.com/kissane","twitterHandle":"kissane","link":null,"childContentfulAuthorBiographyTextNode":{"childMarkdownRemark":{"html":"<p>Erin Kissane is a co-founder of the COVID Tracking Project, and the project’s managing editor. </p>"}},"headshot":{"file":{"fileName":"aea_speaker_erin-kissane_profile-1.jpg"},"resize":{"width":200,"height":200,"src":"//images.ctfassets.net/o2ll9t4ee8tq/2WCYvp5xysZve5yeyurzES/0fee217257ef4d15fdead7b811310be7/aea_speaker_erin-kissane_profile-1.jpg?w=200&fl=progressive&q=50"}}}],"title":"The Decisions We Made","lede":{"lede":"Looking back at what made The COVID Tracking Project work."}},{"slug":"perils-of-automatically-collecting-covid-data","publishDate":"April 22, 2021","authors":[{"name":"Theo Michel","twitterLink":"https://twitter.com/theotayo","twitterHandle":"theotayo","link":null,"childContentfulAuthorBiographyTextNode":{"childMarkdownRemark":{"html":"<p>Theo Michel is a Project Manager and contributor to Data Quality efforts at the Covid Tracking Project.</p>"}},"headshot":{"file":{"fileName":"00100sPORTRAIT_00100_BURST20191228212428376_COVER4.jpeg"},"resize":{"width":200,"height":200,"src":"//images.ctfassets.net/o2ll9t4ee8tq/7MDmEiBhy8tbupgfoGHUxX/fc97cac5c6b3a195e52ba5f045576128/00100sPORTRAIT_00100_BURST20191228212428376_COVER4.jpeg?w=200&fl=progressive&q=50"}}},{"name":"Rebma","twitterLink":null,"twitterHandle":null,"link":null,"childContentfulAuthorBiographyTextNode":{"childMarkdownRemark":{"html":"<p>Rebma is an infrastructure engineer who likes puzzles, solving problems, and who did the data scripting and automation for The Covid Tracking Project.</p>"}},"headshot":{"file":{"fileName":"mob_tome_square.png"},"resize":{"width":200,"height":200,"src":"//images.ctfassets.net/o2ll9t4ee8tq/224EhYZvbAAv2Ld2IqfuTn/e223961d2702b39ba82dc52bf6ed67a6/mob_tome_square.png?w=200&fl=progressive&q=50"}}}],"title":"(The Perils of) Automatically Collecting COVID Data ","lede":{"lede":"Automating data collection published by the states was an adventure full of trickery and misfortune."}},{"slug":"how-we-entered-covid-19-testing-outcomes-data","publishDate":"April 28, 2021","authors":[{"name":"Hannah Hoffman","twitterLink":null,"twitterHandle":null,"link":null,"childContentfulAuthorBiographyTextNode":{"childMarkdownRemark":{"html":"<p>Hannah Hoffman is a data entry and data quality shift lead at The COVID Tracking Project and a student in the Washington, DC area.</p>"}},"headshot":{"file":{"fileName":"Image from iOS (1).jpg"},"resize":{"width":200,"height":250,"src":"//images.ctfassets.net/o2ll9t4ee8tq/3dKAXYqEcRf9qijDkYS4wZ/8732b5f33b7023993ef222f3ed9c6e11/Image_from_iOS__1_.jpg?w=200&fl=progressive&q=50"}}}],"title":"How We Entered COVID-19 Testing and Outcomes Data Every Day for a Year","lede":{"lede":"We set up a set of roles and a shift system to carefully gather and inspect the data we published. \n"}}]}],"blogContent":{"raw":"{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The COVID Tracking Project began collecting data in March 2020, with a few volunteers entering three metrics for 50 states into a spreadsheet manually. It grew into a large organization collecting data across 35 metrics for 56 states and territories up until March 2021. For much of the past year, this data was the most complete COVID-19 dataset across all US states.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Because of the heterogeneous and ever-changing nature of states’ data reporting, manual data collection remained at the core of our process throughout the year. However, \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"our data collection process did evolve to include significant automation, in two ways\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\". One was to double-check human entry of numeric data: While our primary daily collection from each state relied on people, we used automation to verify the manually entered values where possible. The second automation win was when there was a lot of data that was changing frequently. This was the case for time series data when whole time series were continuously updated by the states. These updated time series were automatically fetched daily and used to augment daily collection checks, and for special research projects.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"In this post, we’ll dig into the details of how the automation worked and the issues that we encountered, both from a technical and procedural perspective.\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" We’ll first cover the actual collection of the data, and then the verification and application of metadata, how we kept the data and metadata fresh, and how best to use the data. Post-processing—such as anomaly detection—is outside of the scope of this post, but it is enabled by the creation of a unified view of data that is clearly and correctly tagged. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The data collection pipeline (or really… a series of tubes) \",\"nodeType\":\"text\"}],\"nodeType\":\"heading-3\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The process of collecting the data started with identifying the data sources. This was a manual process of looking for pandemic data on states’ data portals, dashboards, and pandemic information sites. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Many states maintain an open data portal (e.g., \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://data.ca.gov/\"},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"underline\"}],\"value\":\"data.ca.gov\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\"). Data portals aim to provide well structured and annotated data, through APIs (i.e., CKAN, Socrata, and ArcGIS) that allow for complex querying and aggregations. However, \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"the existence of data portals did not guarantee that COVID-19 data was available, and, if it was available, that it was well documented and tagged or even correct\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\". For example, we found that some states’ data portals included placeholder data with random numbers, while other states served their public data from datasets that still had “test_this” in the name. Out of 56 states and jurisdictions, we found that only 9 states had COVID-19 data available through their data portals. (Those states were Alaska, California, Connecticut, Colorado, Florida, Indiana, Maryland, New York, and Vermont).\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"So, most states’ data portals were not useful for identifying data sources, but there was some good news: \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"we were able to extract data from the (often undocumented) APIs and other data sources that backed state dashboards\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\". This was especially easy for states that used ArcGIS to build their dashboards since ArcGIS natively exposes all underlying data through APIs. But we were also able to find underlying APIs or data sources for states that built their own dashboards (such as Delaware, Illinois, and New Mexico). The downside of these dashboard-backing data sources is that they were not documented, and while in some cases (especially ArcGIS) the API provided fields for data definitions, the states usually did not fill them in. Often the original meaning of a field changed over time (e.g., a metric called “tested” started as a metric for tested individuals and later became a record of test specimens). \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"To make matters worse, \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"there were a couple dashboard solutions that stymied us: Tableau and PowerBI\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\". These are quite popular with states, but they do not expose underlying APIs. Instead, the software seems designed to obfuscate the data. These dashboarding solutions do support some forms of data download (e.g. via CSV) but are not configured by default to allow downloads, and state dashboard administrators often did not enable those features. The end result was that data in Tableau and PowerBI was some of the least accessible to automation, even using great libraries such as \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"code\"}],\"value\":\"TableauScraper\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" and custom JavaScript clickers.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Finally, some states provided their data in PDF documents or plain web pages rather than dashboards. These formats were not designed for data extraction, and scraping them was quite susceptible to changes in the source. But because of their popularity, there are many tools available for scraping. In these cases, we wrote custom scrapers using libraries such as \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"code\"}],\"value\":\"BeautifulSoup\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" and \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"code\"}],\"value\":\"pdfminer.six\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" or \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"code\"}],\"value\":\"pyPdf4\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" to extract the data. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"In the end, this was the approximate breakdown of data sources:\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"6zQqDs2rhFReLnSqHUp6eq\",\"type\":\"Link\",\"linkType\":\"Entry\"}}},\"content\":[],\"nodeType\":\"embedded-entry-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"It’s worth noting that \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"we reached out to states to ask them to share their data in more machine-readable formats, and had some success. However, the process was slow, and the results were limited. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"After locating these data sources, we stored them in a YAML configuration file, with each source following this pattern: \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{\"target\":{\"sys\":{\"id\":\"LWzTjJqG5x25msT7MVCOO\",\"type\":\"Link\",\"linkType\":\"Entry\"}}},\"content\":[],\"nodeType\":\"embedded-entry-block\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"We implemented a Python project that iterated over the sources, made the request, and parsed the responses. The aggregated and tagged responses were then uploaded to a Google Spreadsheet (to assist with manual data collection) and stored in a database (for other projects). It was scheduled with cron, and the frequency depended on the use case. For data that people reviewed interactively in order to catch updates and validate data entry, we needed the data very fresh so we decided on a six minute frequency. For data for special projects (i.e., fetching time series), we scheduled it to run a few times a day.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Validating, tagging, and storing\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-3\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"The previous section described the technical part of collecting the data, which is complicated, but at least could be naturally described in code, which makes it the easy part. Next was the mentally labor intensive and iterative process of making sure that the data we had was the \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"italic\"}],\"value\":\"right\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" data, and putting it into a storage schema that allowed for analysis (e.g., for comparisons between states).\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"The central problem we were trying to solve was to align terminology and definitions used by each state. \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\"For example, when a state provided a number of  “cases,” does that mean people with a positive PCR test? Or people showing symptoms and exposure to a known COVID-19 patient? This process required reviewing the sources, finding notes in data dictionaries, reading all dashboard clarification text, reaching out directly to government contacts, and then “tagging” each metric appropriately in our data to describe the real meaning in a consistent way. We found that the quality of the tagging was extremely important to the quality of the resulting dataset.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"This tagging work was also a continuous process—with 56 independent entities, changes to the meaning of data points happened on a daily basis. \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"The tags for metrics changed for many reasons, with or without the changes to the underlying source.\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" For example, tags may have been updated because we found a new piece of information about the published numbers; or the state updated their data dictionary and added clarifications; or CSTE updated their guidelines; or because we refined our tagging (e.g., distinguishing between test specimens versus test encounters) and needed to make adjustments.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"We found that it was important to store data in a form that was clearly tagged, and as close as possible to the original data reported by states. This way when tagging changes occured, we didn’t have to “unwind” the data to incorporate the change. For example, if we wanted to publish a value that we computed by summing two other values, we focused on storing those two values rather than the calculated number.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Serving the data\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-3\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"In addition to collecting and validating the data, we of course wanted to share it out for others to use, and thus we became a data provider. \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"We did our best not to reproduce the same problems that we saw from the states when serving the data, aiming to make our data accessible, transparent, and clearly annotated.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"First, we served the data in both CSV and JSON formats via a REST API. These formats are the most transparent and universally useful. JSON is particularly appropriate because it allows for metadata to sit alongside data at multiple levels.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Next, we were careful to provide detailed notes along with the data, describing the meaning of each datapoint for each state, and enumerating any changes that we made to the data. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Finally, we kept detailed logs of changes to both data and tagging, so that anyone could follow along with our changes. Manual data changes were logged as \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://github.com/COVID19Tracking/issues/issues?page=45&q=is%3Aissue+is%3Aclosed\"},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"underline\"}],\"value\":\"issues in GitHub\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\", and changes to tagging were logged as part of the history of the \",\"nodeType\":\"text\"},{\"data\":{\"uri\":\"https://github.com/COVID19Tracking/covid19-datafetcher/commits/master/dataset/states/mappings.yaml\"},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"underline\"}],\"value\":\"YAML configuration file in GitHub\",\"nodeType\":\"text\"}],\"nodeType\":\"hyperlink\"},{\"data\":{},\"marks\":[],\"value\":\".\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Keeping data up to date\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-3\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Our next concern was how to keep the data up to date. This was a repetitive process of finding and retiring data sources, (re)tagging them, and keeping an eye on updates and modifications to existing sources.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"Very few states notified ahead of time about changes to their datasets.\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\" For example, Vermont and Colorado had good messaging about changes, but most other states made changes without notifications, so it was important to be able to detect the changes as fast as possible.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Some breaking changes could be detected automatically. For example, we kept a log of how many values were fetched for each state, and how many values fetched overall. Watching for changes in these numbers was a quick and simple way to be notified about values we failed to fetch. \",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"But \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"a bigger challenge we had was dealing with silent failures\",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[],\"value\":\". For example, a data source would become stale but still available, leading us to keep pulling stale data without realizing it. Or the underlying meaning of the data would change but we had no way of knowing, and therefore didn’t know that we needed to update our tagging to match.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"This is a big part of why we made our scraping logic as simple as possible. In most software development practices, it’s desirable to make the code robust by handling errors and changes. But in this case, \",\"nodeType\":\"text\"},{\"data\":{},\"marks\":[{\"type\":\"bold\"}],\"value\":\"we decided that failing fast and then fixing each issue after a manual review was the better approach.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Unfortunately, the net result was that in order to keep the automation working, we had to perform regular manual checks of the data source, looking for changes and updates as well as keep an eye out for better sources. This monitoring was largely incorporated into our manual data collection process. Absent that ongoing review, the data collection process would have broken within a week.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Lessons from a year of automation\",\"nodeType\":\"text\"}],\"nodeType\":\"heading-2\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"To summarize, here are the key lessons we learned when trying to automate the collection of COVID-19 data from states’ websites. We hope that these lessons will be useful to others collecting COVID data from states. We believe many of these lessons are likely also applicable to automating data collection for any large set of heterogeneous sources.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Be prepared for the significant amount of manual work that is required to gather the data sources.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Be very flexible on formats. Waiting for states to expose APIs can be futile. Resorting to scraping is inevitable.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Contact your data providers as early as possible to ask for improvements to the formats. But don’t count on those improvements—look for workarounds instead.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Take your time to do the work required to understand the meaning of the data at each data source. Carefully analyze the meaning of the data—don’t just take the name of a field and assume a meaning. This must be an ongoing process.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Store the original data when possible (e.g., zip, json, csv) and within your own database. Store the data in a common format that’s as close to the original as practical, without any logic applied.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"If you serve the aggregated data that you’ve collected so that others can use it, use a standard data format and include all the metadata that you have.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"},{\"data\":{},\"content\":[{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"Reserve plenty of time and manual resources to keep the data and the metadata fresh.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"list-item\"}],\"nodeType\":\"ordered-list\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[{\"type\":\"italic\"}],\"value\":\"Additional contributions from Michal Mart.\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"},{\"data\":{},\"content\":[{\"data\":{},\"marks\":[],\"value\":\"\",\"nodeType\":\"text\"}],\"nodeType\":\"paragraph\"}],\"nodeType\":\"document\"}","references":[{"__typename":"ContentfulContentBlockTable","id":"886efa07-ab04-50b1-a5e7-272347c7bf76","contentful_id":"6zQqDs2rhFReLnSqHUp6eq","table":{"table":"Source Category | Examples | % of CTP Sources\nAPI | ArcGIS, CKAN, Socrata, DKAN | 65%\nStructured Data | CSV, JSON, XLSX | 22%\nRequire Custom Scrapers | PDF, HTML, Tableau, PowerBI | 13%"},"caption":null},{"__typename":"ContentfulContentBlockMarkdown","id":"b678811f-879a-5ad3-9cbd-81189de74382","contentful_id":"LWzTjJqG5x25msT7MVCOO","childContentfulContentBlockMarkdownContentTextNode":{"childMarkdownRemark":{"html":"<pre><code>url: https://data.ca.gov/api/3/action/datastore_search\nparams: {resource_id: b6648a0d-ff0a-4111-b80b-febda2ac9e09, \nlimit: 1, sort: date desc}\ntype: ckan\ndescription: PCR Testing (total)\n</code></pre>"}}}]},"featuredImage":null,"slug":"perils-of-automatically-collecting-covid-data","lede":{"lede":"Automating data collection published by the states was an adventure full of trickery and misfortune."},"publishDate":"April 22, 2021","chartData":null}},"pageContext":{"id":"7a5356b3-ea0c-5367-862a-2198cfcdd7ec","contentful_id":"6klBN04FVKSXfzNT93rm3J","slug":"perils-of-automatically-collecting-covid-data","overrideBlogPage":null,"overrideBlogPath":null,"latestDate":"2021-03-07","sevenDaysAgo":"2021-02-28","fourteenDaysAgo":"2021-02-21","twentyEightDaysAgo":"2021-02-07","ninetyDaysAgo":"2020-12-07"}},"staticQueryHashes":["1039529027","1271460761","2250803104","3649515864","4121435362","775579208"]}